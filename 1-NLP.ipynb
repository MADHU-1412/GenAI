{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6ce71db-65c0-46d8-bbfa-255dcaf840bb",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e8c1718-e874-4870-9c2b-92dd97473fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in e:\\code\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in e:\\code\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in e:\\code\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\code\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in e:\\code\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in e:\\code\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99cb5153-ac1d-4fe6-b550-7361b363f49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"\n",
    "Hi Everyone! My name is Madhupriya. I've completed my Bachelor of Technology in Information Technology.\n",
    "Now I'm an Artifical Intelligence Engineer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5686102f-0b0b-4871-8c8c-a22eef84464e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hi Everyone! My name is Madhupriya. I've completed my Bachelor of Technology in Information Technology.\n",
      "Now I'm an Artifical Intelligence Engineer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28b34202-695a-44d4-84a2-07e99d172f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHi Everyone! My name is Madhupriya. I've completed my Bachelor of Technology in Information Technology.\\nNow I'm an Artifical Intelligence Engineer.\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f90139ef-8742-4959-a7ae-cf530144801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization \n",
    "# para to sentence \n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b07ca6ea-b84f-4cf6-a4b9-a3df56a91a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61edfcb9-82ba-4bec-864c-d0075fbbb160",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d34d9016-49fc-4a8d-92f1-8701f1d11db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nHi Everyone!',\n",
       " 'My name is Madhupriya.',\n",
       " \"I've completed my Bachelor of Technology in Information Technology.\",\n",
       " \"Now I'm an Artifical Intelligence Engineer.\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7914a16-92e8-4352-bb93-399f00c76c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26e81e4c-8629-4408-9e20-36cd8f02752d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hi Everyone!\n",
      "My name is Madhupriya.\n",
      "I've completed my Bachelor of Technology in Information Technology.\n",
      "Now I'm an Artifical Intelligence Engineer.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fadb23a-2ae4-4145-8a64-2390a3d53365",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tokenization\n",
    "# paragraps/sentence ----> Words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd1afe13-6c90-48cf-8d5d-2bf788e28550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'Everyone',\n",
       " '!',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Madhupriya',\n",
       " '.',\n",
       " 'I',\n",
       " \"'ve\",\n",
       " 'completed',\n",
       " 'my',\n",
       " 'Bachelor',\n",
       " 'of',\n",
       " 'Technology',\n",
       " 'in',\n",
       " 'Information',\n",
       " 'Technology',\n",
       " '.',\n",
       " 'Now',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'an',\n",
       " 'Artifical',\n",
       " 'Intelligence',\n",
       " 'Engineer',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45d39a61-6bef-4c78-b9e4-559f07531540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Everyone', '!']\n",
      "['My', 'name', 'is', 'Madhupriya', '.']\n",
      "['I', \"'ve\", 'completed', 'my', 'Bachelor', 'of', 'Technology', 'in', 'Information', 'Technology', '.']\n",
      "['Now', 'I', \"'m\", 'an', 'Artifical', 'Intelligence', 'Engineer', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82bcf08b-6071-4a2e-98e1-7a1574fdca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abe57f14-159f-4888-8d52-bb5eed9216c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Everyone', '!']\n",
      "['My', 'name', 'is', 'Madhupriya', '.']\n",
      "['I', \"'\", 've', 'completed', 'my', 'Bachelor', 'of', 'Technology', 'in', 'Information', 'Technology', '.']\n",
      "['Now', 'I', \"'\", 'm', 'an', 'Artifical', 'Intelligence', 'Engineer', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(wordpunct_tokenize(sentence))\n",
    "# apostrope difference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd932794-9f57-4a6b-aa46-0c580a736b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1581eaf5-436f-4118-8c6a-c3cefd54132b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'Everyone',\n",
       " '!',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Madhupriya.',\n",
       " 'I',\n",
       " \"'ve\",\n",
       " 'completed',\n",
       " 'my',\n",
       " 'Bachelor',\n",
       " 'of',\n",
       " 'Technology',\n",
       " 'in',\n",
       " 'Information',\n",
       " 'Technology.',\n",
       " 'Now',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'an',\n",
       " 'Artifical',\n",
       " 'Intelligence',\n",
       " 'Engineer',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da696894-2566-4c65-a341-d73a10a233f4",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e69cd32-6859-4db6-8c18-c0b5d406d5b2",
   "metadata": {},
   "source": [
    "**Stemming** is the process of reducing words to their root form by chopping off prefixes or suffixes, often without caring about grammar.\n",
    "**Example:** *â€œrunning, runs, runnerâ€ â†’ â€œrunâ€* (using a stemmer like Porter).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62369ed1-fa8f-4cc7-a873-ef56cd733722",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"programming\", \"programmed\", \"programs\",\n",
    "    \"eaten\", \"eating\", \"eats\",\n",
    "    \"running\", \"runs\", \"ran\",\n",
    "    \"writing\", \"writes\", \"written\",\n",
    "    \"driving\",  \"drives\",\n",
    "    \"speaking\", \"spoken\", \"speaks\",\n",
    "    \"swimming\",  \"swims\",\"study\",\n",
    "    \"building\", \"built\", \"builds\",\n",
    "    \"breaking\", \"broken\", \"breaks\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36ffaaef-b867-41ab-9483-dce7be69bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "steming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54700a36-8642-40f6-9245-45ff72334f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "programming--->program\n",
      "programmed--->program\n",
      "programs--->program\n",
      "eaten--->eaten\n",
      "eating--->eat\n",
      "eats--->eat\n",
      "running--->run\n",
      "runs--->run\n",
      "ran--->ran\n",
      "writing--->write\n",
      "writes--->write\n",
      "written--->written\n",
      "driving--->drive\n",
      "drives--->drive\n",
      "speaking--->speak\n",
      "spoken--->spoken\n",
      "speaks--->speak\n",
      "swimming--->swim\n",
      "swims--->swim\n",
      "study--->studi\n",
      "building--->build\n",
      "built--->built\n",
      "builds--->build\n",
      "breaking--->break\n",
      "broken--->broken\n",
      "breaks--->break\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \"--->\" + steming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b62af0ab-c580-4417-8c17-428d51022c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"study--->studi\" --> it major disadvantge because it changes the meaning of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bda4e9a6-039c-4cdb-917c-dd4d293ee2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steming.stem('Congratulation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f3477a-d834-4663-a17c-7e893d6b944e",
   "metadata": {},
   "source": [
    "### RegexStemmer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794e9e74-de3a-44c5-a0a0-98695ef7b727",
   "metadata": {},
   "source": [
    "**RegexStemmer** reduces words by removing patterns using regular expressions (like stripping *ing, ed, s*).\n",
    "**Example:** with a rule `r\"(ing|ed|s)$\"` â†’ *â€œrunning, played, carsâ€ â†’ â€œrunn, play, carâ€*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6dcd916-ef7e-4bc4-b7b0-1979cc6f639f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "reg_stem = RegexpStemmer('ing$|es$|s$|able$', min = 4)\n",
    "reg_stem.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2c5f280-3f99-4da8-a025-fadd35dc57f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingeat'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stem.stem('ingeating') #it will remove only at the last"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7874b91-1550-409e-b70b-3a4e17751a07",
   "metadata": {},
   "source": [
    "### Snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f41ac564-7037-44b7-9f60-3927ea589683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stude'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It performs better than PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')\n",
    "snowball.stem(\"studing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba0ecbad-2b16-4328-8654-0f6bb18bdbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "programming-->program\n",
      "programmed-->program\n",
      "programs-->program\n",
      "eaten-->eaten\n",
      "eating-->eat\n",
      "eats-->eat\n",
      "running-->run\n",
      "runs-->run\n",
      "ran-->ran\n",
      "writing-->write\n",
      "writes-->write\n",
      "written-->written\n",
      "driving-->drive\n",
      "drives-->drive\n",
      "speaking-->speak\n",
      "spoken-->spoken\n",
      "speaks-->speak\n",
      "swimming-->swim\n",
      "swims-->swim\n",
      "study-->studi\n",
      "building-->build\n",
      "built-->built\n",
      "builds-->build\n",
      "breaking-->break\n",
      "broken-->broken\n",
      "breaks-->break\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \"-->\" + snowball.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9aafc6-e935-4052-bb11-22313125c09e",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5880f86-977e-4f44-b5bb-5a7683f6cdec",
   "metadata": {},
   "source": [
    "**Lemmatization** reduces a word to its **dictionary base form** (lemma) using vocabulary and grammar rules.\n",
    "**Example:** *â€œrunning, ranâ€ â†’ â€œrunâ€*, *â€œbetterâ€ â†’ â€œgoodâ€*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fc2e1fd-233f-465f-a9ba-2172b5e9ec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "648b1d08-f5ef-41b6-bf0c-d1b9f3cfd521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')   # optional but recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6ab6a2f-48b1-4bec-b0be-02ea431fd7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize(\"going\", pos = 'v')\n",
    "\n",
    "#noun - n\n",
    "#verb - v\n",
    "#adverb - r\n",
    "#adjective - a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bdfe1a51-a45b-4419-9a75-833f574a0c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'study'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize(\"studying\", pos = 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22640e08-b43f-460b-81db-71029ce66b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "programming-->program\n",
      "programmed-->program\n",
      "programs-->program\n",
      "eaten-->eat\n",
      "eating-->eat\n",
      "eats-->eat\n",
      "running-->run\n",
      "runs-->run\n",
      "ran-->run\n",
      "writing-->write\n",
      "writes-->write\n",
      "written-->write\n",
      "driving-->drive\n",
      "drives-->drive\n",
      "speaking-->speak\n",
      "spoken-->speak\n",
      "speaks-->speak\n",
      "swimming-->swim\n",
      "swims-->swim\n",
      "study-->study\n",
      "building-->build\n",
      "built-->build\n",
      "builds-->build\n",
      "breaking-->break\n",
      "broken-->break\n",
      "breaks-->break\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \"-->\" + lemma.lemmatize(word, pos = 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39854ac4-b649-413e-921c-cf99a94da6ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Congratulation'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize(\"Congratulation\", pos = 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdc3b3c-9cff-4134-85d4-35fe01744c93",
   "metadata": {},
   "source": [
    "# StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2360412a-7cd8-46f8-b26c-09c66cc7242b",
   "metadata": {},
   "source": [
    "**stopwords** are common words like *â€œis,â€ â€œthe,â€ â€œand,â€ â€œinâ€* that usually donâ€™t add much meaning to a sentence.\n",
    "They are often removed during text preprocessing to make models focus on more important, meaningful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d818b38a-f2df-4833-9099-2b5a5888ee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c102ecc2-2675-4bbe-8fa0-8bf31d94d112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f5e4cb9-f213-494b-af48-2adbf21c22c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')\n",
    "# good to create your own stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c53dc9e1-a8ba-4e8d-9b87-f1bdbc7fb52f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aber',\n",
       " 'alle',\n",
       " 'allem',\n",
       " 'allen',\n",
       " 'aller',\n",
       " 'alles',\n",
       " 'als',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ander',\n",
       " 'andere',\n",
       " 'anderem',\n",
       " 'anderen',\n",
       " 'anderer',\n",
       " 'anderes',\n",
       " 'anderm',\n",
       " 'andern',\n",
       " 'anderr',\n",
       " 'anders',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'aus',\n",
       " 'bei',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bist',\n",
       " 'da',\n",
       " 'damit',\n",
       " 'dann',\n",
       " 'der',\n",
       " 'den',\n",
       " 'des',\n",
       " 'dem',\n",
       " 'die',\n",
       " 'das',\n",
       " 'dass',\n",
       " 'daÃŸ',\n",
       " 'derselbe',\n",
       " 'derselben',\n",
       " 'denselben',\n",
       " 'desselben',\n",
       " 'demselben',\n",
       " 'dieselbe',\n",
       " 'dieselben',\n",
       " 'dasselbe',\n",
       " 'dazu',\n",
       " 'dein',\n",
       " 'deine',\n",
       " 'deinem',\n",
       " 'deinen',\n",
       " 'deiner',\n",
       " 'deines',\n",
       " 'denn',\n",
       " 'derer',\n",
       " 'dessen',\n",
       " 'dich',\n",
       " 'dir',\n",
       " 'du',\n",
       " 'dies',\n",
       " 'diese',\n",
       " 'diesem',\n",
       " 'diesen',\n",
       " 'dieser',\n",
       " 'dieses',\n",
       " 'doch',\n",
       " 'dort',\n",
       " 'durch',\n",
       " 'ein',\n",
       " 'eine',\n",
       " 'einem',\n",
       " 'einen',\n",
       " 'einer',\n",
       " 'eines',\n",
       " 'einig',\n",
       " 'einige',\n",
       " 'einigem',\n",
       " 'einigen',\n",
       " 'einiger',\n",
       " 'einiges',\n",
       " 'einmal',\n",
       " 'er',\n",
       " 'ihn',\n",
       " 'ihm',\n",
       " 'es',\n",
       " 'etwas',\n",
       " 'euer',\n",
       " 'eure',\n",
       " 'eurem',\n",
       " 'euren',\n",
       " 'eurer',\n",
       " 'eures',\n",
       " 'fÃ¼r',\n",
       " 'gegen',\n",
       " 'gewesen',\n",
       " 'hab',\n",
       " 'habe',\n",
       " 'haben',\n",
       " 'hat',\n",
       " 'hatte',\n",
       " 'hatten',\n",
       " 'hier',\n",
       " 'hin',\n",
       " 'hinter',\n",
       " 'ich',\n",
       " 'mich',\n",
       " 'mir',\n",
       " 'ihr',\n",
       " 'ihre',\n",
       " 'ihrem',\n",
       " 'ihren',\n",
       " 'ihrer',\n",
       " 'ihres',\n",
       " 'euch',\n",
       " 'im',\n",
       " 'in',\n",
       " 'indem',\n",
       " 'ins',\n",
       " 'ist',\n",
       " 'jede',\n",
       " 'jedem',\n",
       " 'jeden',\n",
       " 'jeder',\n",
       " 'jedes',\n",
       " 'jene',\n",
       " 'jenem',\n",
       " 'jenen',\n",
       " 'jener',\n",
       " 'jenes',\n",
       " 'jetzt',\n",
       " 'kann',\n",
       " 'kein',\n",
       " 'keine',\n",
       " 'keinem',\n",
       " 'keinen',\n",
       " 'keiner',\n",
       " 'keines',\n",
       " 'kÃ¶nnen',\n",
       " 'kÃ¶nnte',\n",
       " 'machen',\n",
       " 'man',\n",
       " 'manche',\n",
       " 'manchem',\n",
       " 'manchen',\n",
       " 'mancher',\n",
       " 'manches',\n",
       " 'mein',\n",
       " 'meine',\n",
       " 'meinem',\n",
       " 'meinen',\n",
       " 'meiner',\n",
       " 'meines',\n",
       " 'mit',\n",
       " 'muss',\n",
       " 'musste',\n",
       " 'nach',\n",
       " 'nicht',\n",
       " 'nichts',\n",
       " 'noch',\n",
       " 'nun',\n",
       " 'nur',\n",
       " 'ob',\n",
       " 'oder',\n",
       " 'ohne',\n",
       " 'sehr',\n",
       " 'sein',\n",
       " 'seine',\n",
       " 'seinem',\n",
       " 'seinen',\n",
       " 'seiner',\n",
       " 'seines',\n",
       " 'selbst',\n",
       " 'sich',\n",
       " 'sie',\n",
       " 'ihnen',\n",
       " 'sind',\n",
       " 'so',\n",
       " 'solche',\n",
       " 'solchem',\n",
       " 'solchen',\n",
       " 'solcher',\n",
       " 'solches',\n",
       " 'soll',\n",
       " 'sollte',\n",
       " 'sondern',\n",
       " 'sonst',\n",
       " 'Ã¼ber',\n",
       " 'um',\n",
       " 'und',\n",
       " 'uns',\n",
       " 'unsere',\n",
       " 'unserem',\n",
       " 'unseren',\n",
       " 'unser',\n",
       " 'unseres',\n",
       " 'unter',\n",
       " 'viel',\n",
       " 'vom',\n",
       " 'von',\n",
       " 'vor',\n",
       " 'wÃ¤hrend',\n",
       " 'war',\n",
       " 'waren',\n",
       " 'warst',\n",
       " 'was',\n",
       " 'weg',\n",
       " 'weil',\n",
       " 'weiter',\n",
       " 'welche',\n",
       " 'welchem',\n",
       " 'welchen',\n",
       " 'welcher',\n",
       " 'welches',\n",
       " 'wenn',\n",
       " 'werde',\n",
       " 'werden',\n",
       " 'wie',\n",
       " 'wieder',\n",
       " 'will',\n",
       " 'wir',\n",
       " 'wird',\n",
       " 'wirst',\n",
       " 'wo',\n",
       " 'wollen',\n",
       " 'wollte',\n",
       " 'wÃ¼rde',\n",
       " 'wÃ¼rden',\n",
       " 'zu',\n",
       " 'zum',\n",
       " 'zur',\n",
       " 'zwar',\n",
       " 'zwischen']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9deca538-6fde-409f-b05f-d72af13fabf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph =\"\"\" My dear young friends,\n",
    "I see in your eyes the dreams of a great nation.\n",
    "Each one of you has the power to shape the future of India through knowledge and hard work.\n",
    "Dream big, because small dreams have no power to move the human heart.\n",
    "When challenges come, do not fear them â€” they are stepping stones to success.\n",
    "Discipline, dedication, and determination will take you to great heights.\n",
    "Education is the most powerful weapon to transform society.\n",
    "Believe in yourself even when the world doubts you.\n",
    "Work not for personal success alone, but for the progress of our nation.\n",
    "Together, let us build a developed, strong, and peaceful India.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ccf6ca4-a1f6-47fc-9db1-6927c6c84fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "58a28a2d-e970-48b1-9817-f60b554d8fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencess = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a906b88-d22e-4d56-ae72-7579ad4f84b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentencess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "135f1f85-eeec-47f1-967b-6fcbebe036c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' My dear young friends,\\nI see in your eyes the dreams of a great nation.',\n",
       " 'Each one of you has the power to shape the future of India through knowledge and hard work.',\n",
       " 'Dream big, because small dreams have no power to move the human heart.',\n",
       " 'When challenges come, do not fear them â€” they are stepping stones to success.',\n",
       " 'Discipline, dedication, and determination will take you to great heights.',\n",
       " 'Education is the most powerful weapon to transform society.',\n",
       " 'Believe in yourself even when the world doubts you.',\n",
       " 'Work not for personal success alone, but for the progress of our nation.',\n",
       " 'Together, let us build a developed, strong, and peaceful India.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentencess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bed901f8-e0fc-45f2-a81c-8d4f8489210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Stopwords -> Filter -> then Apply Stemming \n",
    "for i in range(len(sentencess)):\n",
    "    words = nltk.word_tokenize(sentencess[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentencess[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a91c222-fa55-4c18-9696-9c3dd4b05707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my dear young friend , i see eye dream great nation .',\n",
       " 'each one power shape futur india knowledg hard work .',\n",
       " 'dream big , small dream power move human heart .',\n",
       " 'when challeng come , fear â€” step stone success .',\n",
       " 'disciplin , dedic , determin take great height .',\n",
       " 'educ power weapon transform societi .',\n",
       " 'believ even world doubt .',\n",
       " 'work person success alon , progress nation .',\n",
       " 'togeth , let us build develop , strong , peac india .']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentencess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9a41056f-d8af-468d-a90c-0d6245ddf4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming does not look very good so we are gonna apply snowball stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "snow_ball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d5e743e4-83a5-4e17-ab8e-84a3cb5c9cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' My dear young friends,\\nI see in your eyes the dreams of a great nation.\\nEach one of you has the power to shape the future of India through knowledge and hard work.\\nDream big, because small dreams have no power to move the human heart.\\nWhen challenges come, do not fear them â€” they are stepping stones to success.\\nDiscipline, dedication, and determination will take you to great heights.\\nEducation is the most powerful weapon to transform society.\\nBelieve in yourself even when the world doubts you.\\nWork not for personal success alone, but for the progress of our nation.\\nTogether, let us build a developed, strong, and peaceful India.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ed54116-ddcc-4e9e-a678-6ffa7ec36915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "stemed_lemma = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f2b5f67a-6ff8-4417-97b8-787e6b162938",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stemed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stemed\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stemed' is not defined"
     ]
    }
   ],
   "source": [
    "stemed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f39f7c-576a-478c-afcc-da14aca52e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(stemed)):\n",
    "    words = nltk.word_tokenize(stemed[i])\n",
    "    words = [snowball.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    stemed[i] = ' '.join(words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a577e3f-35e9-41e0-a7ce-97ae3b3be9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e2e0bb-3857-4b57-9cae-d2ea35971218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb66134-a994-4bf6-9981-52fcb1b09fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(stemed_lemma)):\n",
    "    words = nltk.word_tokenize(stemed_lemma[i])\n",
    "    words = [lemma.lemmatize(word, pos = 'v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    stemed_lemma[i] = ' '.join(words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f29352-da2f-4858-8545-4a9ddfb2c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemed_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86a11ee-9f22-4036-add4-46d0af693d38",
   "metadata": {},
   "source": [
    "# Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4b31f6f-b2c4-4410-9d66-2896949b1b31",
   "metadata": {},
   "source": [
    "| Tag      | Meaning                                 | Example         |\n",
    "| -------- | --------------------------------------- | --------------- |\n",
    "| **CC**   | Coordinating conjunction                | and, but, or    |\n",
    "| **CD**   | Cardinal number                         | one, 10, fifty  |\n",
    "| **DT**   | Determiner                              | the, a, this    |\n",
    "| **EX**   | Existential there                       | there is        |\n",
    "| **FW**   | Foreign word                            | bonjour         |\n",
    "| **IN**   | Preposition / subordinating conjunction | in, on, because |\n",
    "| **JJ**   | Adjective                               | beautiful       |\n",
    "| **JJR**  | Adjective, comparative                  | bigger          |\n",
    "| **JJS**  | Adjective, superlative                  | biggest         |\n",
    "| **LS**   | List marker                             | 1), A)          |\n",
    "| **MD**   | Modal verb                              | can, should     |\n",
    "| **NN**   | Noun (singular)                         | dog             |\n",
    "| **NNS**  | Noun (plural)                           | dogs            |\n",
    "| **NNP**  | Proper noun (singular)                  | India           |\n",
    "| **NNPS** | Proper noun (plural)                    | Indians         |\n",
    "| **PDT**  | Predeterminer                           | all, both       |\n",
    "| **POS**  | Possessive ending                       | 's              |\n",
    "| **PRP**  | Personal pronoun                        | I, he, she      |\n",
    "| **PRP$** | Possessive pronoun                      | my, his         |\n",
    "| **RB**   | Adverb                                  | quickly         |\n",
    "| **RBR**  | Adverb comparative                      | faster          |\n",
    "| **RBS**  | Adverb superlative                      | fastest         |\n",
    "| **RP**   | Particle                                | up, off         |\n",
    "| **SYM**  | Symbol                                  | %, $, +         |\n",
    "| **TO**   | to                                      | to go           |\n",
    "| **UH**   | Interjection                            | wow             |\n",
    "| **VB**   | Verb base                               | run             |\n",
    "| **VBD**  | Verb past                               | ran             |\n",
    "| **VBG**  | Verb gerund                             | running         |\n",
    "| **VBN**  | Verb past participle                    | eaten           |\n",
    "| **VBP**  | Verb present                            | run             |\n",
    "| **VBZ**  | Verb present 3rd person                 | runs            |\n",
    "| **WDT**  | Wh-determiner                           | which           |\n",
    "| **WP**   | Wh-pronoun                              | who             |\n",
    "| **WP$**  | Possessive wh-pronoun                   | whose           |\n",
    "| **WRB**  | Wh-adverb                               | where           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b03c46-e748-40e9-b7fd-f6c7aa06060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "para  = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for Indiaâ€™s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isnâ€™t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d21a9-fd05-4fe8-9144-54d15cc22e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "sentence = nltk.sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb04e09-63c3-411a-86f0-d43f7fd72546",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ddd40-caf9-4bb6-9425-d0d49c556ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59af7ec-37b8-4827-8cd5-b14467e4dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec97ae9f-8bda-4af9-9706-514a3cae131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the pos tag \n",
    "for i in range(len(sentence)):\n",
    "    words = nltk.word_tokenize(sentence[i])\n",
    "    words  = [word for word in words if word not in stopwords.words('english')]\n",
    "    pos_tag = nltk.pos_tag(words)\n",
    "    print(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8461e50f-13bb-415b-90fd-2ccaab85106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"Taj Mahal is a beautiful Monument\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbc24c2-380f-41ab-b3d6-1352a7d38c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "linee = line.split()\n",
    "print(nltk.pos_tag(linee))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8fbd5a-4945-445c-a7c6-f9a4fadc7e4a",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e4c467f-a6d7-4053-800d-09b66ecc8ff9",
   "metadata": {},
   "source": [
    "NLP task that finds and labels real-world things in text like:\n",
    "\n",
    "ðŸ‘‰ people, places, organizations, dates, money, etc.\n",
    "\n",
    "Example sentence:\n",
    "\n",
    "> *Elon Musk founded Tesla in 2003 in California.*\n",
    "\n",
    "NER output:\n",
    "\n",
    "* Elon Musk â†’ PERSON\n",
    "* Tesla â†’ ORGANIZATION\n",
    "* 2003 â†’ DATE\n",
    "* California â†’ LOCATION\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Common Named Entity Tags (most used in NLP)\n",
    "\n",
    "| Tag          | What it means           | Example               |\n",
    "| ------------ | ----------------------- | --------------------- |\n",
    "| **PERSON**   | Person name             | Rahul, Einstein       |\n",
    "| **ORG**      | Organization            | Google, Microsoft     |\n",
    "| **GPE**      | Country/City/State      | India, Paris          |\n",
    "| **LOC**      | Location                | Himalayas, River Nile |\n",
    "| **DATE**     | Date                    | 2024, Monday          |\n",
    "| **TIME**     | Time                    | 5 PM, morning         |\n",
    "| **MONEY**    | Money                   | $100, â‚¹500            |\n",
    "| **PERCENT**  | Percentage              | 20%                   |\n",
    "| **FAC**      | Buildings/places        | Airport, bridge       |\n",
    "| **PRODUCT**  | Product name            | iPhone, Tesla Model 3 |\n",
    "| **EVENT**    | Events                  | Olympics, World Cup   |\n",
    "| **LAW**      | Law names               | Constitution, IPC     |\n",
    "| **LANGUAGE** | Language                | English, Hindi        |\n",
    "| **NORP**     | Nationalities/Religions | Indian, Christian     |\n",
    "| **QUANTITY** | Measurements            | 5 kg, 10 liters       |\n",
    "| **ORDINAL**  | Order                   | first, second         |\n",
    "| **CARDINAL** | Numbers                 | one, 100              |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Super simple example\n",
    "\n",
    "Sentence:\n",
    "\n",
    "> Apple opened a new office in Bengaluru in 2023.\n",
    "\n",
    "NER:\n",
    "\n",
    "* Apple â†’ ORG\n",
    "* Bengaluru â†’ GPE\n",
    "* 2023 â†’ DATE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1384dac1-3896-466c-8532-abad81ee2662",
   "metadata": {},
   "outputs": [],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67101826-2345-410b-852b-4fa166f54aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = nltk.word_tokenize(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedecded-2a5d-4aea-a2bf-9b0a44cb2df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de7b111-093f-417e-8ee4-62947485a3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('maxent_ne_chunker_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e091362-deec-4b11-9172-f2b0d6d2f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb0c7c6-ad4a-440f-b1a8-fe236f3f9220",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = nltk.pos_tag(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c6c1fb-ef04-42bc-95b1-19b05d764937",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.ne_chunk(tag).draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcc1d47-f3f8-4a82-a7a5-ec92ca487df4",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d2c3a652-084f-4d7d-b692-1b2115c111b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in e:\\code\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in e:\\code\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in e:\\code\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in e:\\code\\lib\\site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92bc1f49-46f8-42bf-b5f9-83963d926f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4ae5c71f-cb45-43ff-8a1f-92693a88dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "#ref: https://huggingface.co/fse/word2vec-google-news-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b0508-be2d-4a42-b91d-04553c88df6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------------------------------------] 1.6% 26.4/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=-------------------------------------------------] 3.3% 54.7/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==------------------------------------------------] 5.1% 85.6/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===-----------------------------------------------] 7.0% 116.8/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====----------------------------------------------] 8.8% 146.9/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====---------------------------------------------] 10.8% 179.2/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======--------------------------------------------] 12.6% 210.3/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======-------------------------------------------] 14.7% 244.2/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========------------------------------------------] 16.6% 275.6/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========-----------------------------------------] 18.8% 312.7/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========----------------------------------------] 20.6% 343.1/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===========---------------------------------------] 22.7% 377.6/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============--------------------------------------] 24.9% 413.3/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============--------------------------------------] 25.8% 428.2/1662.8MB downloaded"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "xv = api.load('word2vec-google-news-300')\n",
    "vec_king = xv['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d11f83-7340-4185-a206-9ef5cc203575",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_king.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcec0df-1261-406a-8e58-bd0f79103c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv['cricket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a472c4be-3582-454f-983a-8c4a5d9d1c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar('Dhoni')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5d96ab-e1e9-4467-a8e7-20b0f3de3fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_Similar('happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a52d4a-469e-4f03-be57-0e79b745b611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9244de13-c934-41b3-8fad-0366f7052bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224d426d-6960-46a4-9072-041716b2f823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
